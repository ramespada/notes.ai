---
layout: lecture
title: "Bibliografía"
---
# Bibliografía 
En esta sección se presentan algunos papers y referencias para profundizar los temas tratados en el curso.


1. 1958 – Rosenblatt. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain”
The birth certificate of neural networks. Introduced the perceptron, a learnable model inspired by neurons. Why it matters:
- First learning algorithm for a neural-like model
- Sparked both excitement and backlash that shaped decades of research

2. 1967 – Cover & Hart “Nearest Neighbor Pattern Classification” 
The quiet foundation of instance-based learning.
Why it matters:
- Formalized the nearest-neighbor idea
- Proved simple methods can be surprisingly powerful
- Still underpins modern metric learning

3. 1986 – Rumelhart, Hinton & Williams. “Learning Representations by Back-Propagating Errors”
The resurrection spell for neural networks.
Why it matters:
- Introduced backpropagation for multi-layer networks
- Made deep learning possible in principle
- One of the most cited papers in computer science

4. 1995 – Vapnik. “The Nature of Statistical Learning Theory”
The theoretical backbone of modern ML.
Why it matters:
- Introduced VC theory and generalization bounds
- Led directly to Support Vector Machines
- Shifted ML toward mathematical rigor

5. 1997 – Hochreiter & Schmidhuber  “Long Short-Term Memory”

Teaching machines to remember.
Why it matters:
- Solved the vanishing gradient problem in RNNs
- Enabled learning over long sequences
- Still used today, even in the transformer era

6. 1998 – LeCun et al. “Gradient-Based Learning Applied to Document Recognition”

The CNN manifesto.
Why it matters:
- Demonstrated convolutional neural networks in practice
- Proved end-to-end learning works for vision
- Powering computer vision for two decades

7. 2006 – Hinton, Osindero & Teh “A Fast Learning Algorithm for Deep Belief Nets”
The spark that reignited deep learning.

Why it matters:
- Showed deep networks could be trained effectively
- Reversed skepticism toward neural nets
- Set the stage for the deep learning boom

8. 2012 – Krizhevsky, Sutskever & Hinton “ImageNet Classification with Deep Convolutional Neural Networks”
The earthquake.
- Why it matters:
- Reduced ImageNet error dramatically overnight
- Proved deep learning beats handcrafted features
- Triggered industry-wide adoption

9. 2014 – Goodfellow et al. “Generative Adversarial Nets”
Two networks enter. Reality leaves.
Why it matters:
- Introduced adversarial training
- Revolutionized generative modeling
- Influenced everything from art to data augmentation

10. 2017 – Vaswani et al. “Attention Is All You Need”
The transformer ignition.
Why it matters:
- Replaced recurrence with attention
- Enabled massive parallelism and scale
- Foundation of modern language and multimodal models



## Miscelaneos:
- [Art of the problem (Youtube) - Artifitial Inteligence](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Welch Labs (Youtube) - Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU)
- [3B1B (Youtube) Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)




































































